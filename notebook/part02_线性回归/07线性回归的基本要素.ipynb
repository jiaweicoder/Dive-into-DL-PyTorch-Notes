{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitpytorchcondadba5b8d482fb49e9a843bab88f901f7d",
   "display_name": "Python 3.7.4 64-bit ('pytorch': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 线性回归的基本要素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.模型定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "线性回归假设输出与各个输入之间是线性关系:\n",
    "\n",
    "$$ \\hat{\\mathrm{y}} = w_{\\mathrm{1}} \\cdot x_{\\mathrm{1}} + w_{\\mathrm{2}} \\cdot x_{\\mathrm{2}} + b $$\n",
    "\n",
    "上述的这种线性关系我们一般称之为模型(model)\n",
    "\n",
    "其中 $w_{\\mathrm{1}}$ 和 $w_{\\mathrm{2}}$ 是权重(weight)，$b$ 是偏差(bias)，且均为标量。它们是线性回归模型的参数(parameter)。\n",
    "\n",
    "$\\hat{\\mathrm{y}}$ 是对真实 $y$ 的预测和估计。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)训练数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集：训练数据集(training data set)又称为训练集(training set)。在房屋价格预测中，它们是多栋房屋的真实售出价格和它们对应的面积和房龄。一栋房屋被称为一个样本(sample)，其真实售出价格被称为标签(label)。用来预测标签的两个因素“面积”和“房龄”被称为特征(feature)。特征用来表征样本的特点。\n",
    "\n",
    "当采集样本数为 $n$ 时，角标为 $i$ 的样本的特征为 $x_{\\mathrm{1}}^{(i)}$ 和 $x_{\\mathrm{2}}^{(i)}$ ，标签为 $y^{(i)}$ 。\n",
    "\n",
    "对于角标为 $i$ 的样本，它的线性回归模型的预测表达式为：\n",
    "\n",
    "$$ \\hat{\\mathrm{y}}^{(i)} = w_{\\mathrm{1}} \\cdot x_{\\mathrm{1}}^{(i)} + w_{\\mathrm{2}} \\cdot x_{\\mathrm{2}}^{(i)} + b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2)损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在模型训练中，我们需要衡量预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。 它在评估角标为 $i$ 的样本误差的表达式为：\n",
    "\n",
    "$$ l^{(i)}(w_{\\mathrm{1}}, w_{\\mathrm{2}}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2 $$\n",
    "\n",
    "若其参数不只 $w_{\\mathrm{1}}$ , $w_{\\mathrm{2}}$ 两个，则可以用矩阵 $\\mathbf{w}$ 表示，此时的样本误差表达式为：\n",
    "\n",
    "$$ l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2 $$\n",
    "\n",
    "通常，我们用训练集中的所有样本误差的平均值来衡量模型的效果，即：\n",
    "\n",
    "$$ L(w_{\\mathrm{1}}, w_{\\mathrm{2}}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(w_{\\mathrm{1}}, w_{\\mathrm{2}}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(w_{\\mathrm{1}} \\cdot x_{\\mathrm{1}}^{(i)} + w_{\\mathrm{2}} \\cdot x_{\\mathrm{2}}^{(i)} + b - y^{(i)}\\right)^2 $$\n",
    "\n",
    "当将 $x_{\\mathrm{1}}^{(i)}$ 和 $x_{\\mathrm{2}}^{(i)}$ 也用矩阵 $\\mathbf{x}^{(i)}$ 表示时，表达式可以写成：\n",
    "\n",
    "$$ L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2 $$\n",
    "\n",
    "在训练模型的过程中，我们希望找到一组模型参数，使训练样本误差的平均值最小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3)优化算法——随机梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解(analytical solution)。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解(numerical solution)。\n",
    "\n",
    "在求数值解的优化算法中，小批量随机梯度下降(mini-batch stochastic gradient descent)在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量(mini-batch) $\\mathcal{B}$ ，然后求小批量中数据样本的平均损失有关模型参数的导数(梯度)，最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。\n",
    "\n",
    "在本次线性回归模型中，模型的每个参数将做如下迭代：\n",
    "\n",
    "$$ w_{\\mathrm{1}} \\leftarrow w_{\\mathrm{1}} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\frac{\\partial{l^{(i)}(w_{\\mathrm{1}}, w_{\\mathrm{2}}, b)}} {\\partial{w_{\\mathrm{1}}}} $$\n",
    "\n",
    "$$ w_{\\mathrm{2}} \\leftarrow w_{\\mathrm{2}} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\frac{\\partial{l^{(i)}(w_{\\mathrm{1}}, w_{\\mathrm{2}}, b)}} {\\partial{w_{\\mathrm{2}}}} $$\n",
    "\n",
    "$$ b \\leftarrow b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\frac{\\partial{l^{(i)}(w_{\\mathrm{1}}, w_{\\mathrm{2}}, b)}} {\\partial{b}} $$\n",
    "\n",
    "写成矩阵形式就是：\n",
    "\n",
    "$$ (\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b) $$\n",
    "\n",
    "在上式中，$|\\mathcal{B}|$ 是每个小批量中的样本个数，即批量大小(batch size)；$\\eta$ 被称作学习率(learning rate)，取正数，在每次优化中，能够学习的步长的大小。\n",
    "\n",
    "这里的批量大小和学习率都是人为设置的参数，我们称之为超参数(hyperparameter)。我们通常说的“调参”，即是调节超参数。\n",
    "\n",
    "总结一下，优化函数的有以下两个步骤：\n",
    "\n",
    "(1)初始化模型参数，一般来说使用随机初始化；\n",
    "\n",
    "(2)我们在数据上迭代多次，通过在负梯度方向移动参数来更新每个参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练完成后，我们将模型参数 $w_{\\mathrm{1}}$ , $w_{\\mathrm{2}}$ , $b$ 在优化算法停止时的值分别是$\\hat{w_{\\mathrm{1}}}$ , $\\hat{w_{\\mathrm{2}}}$, $\\hat{b}$ 。这三个参数不一定是最小化损失函数的最优解，而是最优解的一个近似。我们可以利用线性回归模型 $\\hat{w_{\\mathrm{1}}} \\cdot x_{\\mathrm{1}} + \\hat{w_{\\mathrm{2}}} \\cdot x_{\\mathrm{2}} + \\hat{b}$ 来进行预测。"
   ]
  }
 ]
}