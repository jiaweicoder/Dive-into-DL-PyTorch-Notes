{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09线性回归模型的pytorch简洁实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.导入pytorch框架，并确认其版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0ad10e4970>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.生成数据集\n",
    "在这里生成数据集跟之前的实现中是完全一样的。其中features是训练数据特征,labels是标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "\n",
    "true_w = [2, -3.4]\n",
    "true_b = 4.2\n",
    "\n",
    "features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)\n",
    "labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\n",
    "labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.读取数据集\n",
    "PyTorch提供了data包来读取数据。由于data常用作变量量名,我们将导入的data模块用用Data代替。在每一一次迭代中,我们将随机读取包含10个数据样本的小批量样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "# 将训练数据的特征和标签组合\n",
    "dataset = Data.TensorDataset(features, labels)\n",
    "\n",
    "# 随机读取小批量样本\n",
    "data_iter = Data.DataLoader(\n",
    "    dataset=dataset,            # torch的TensorDataset格式\n",
    "    batch_size=batch_size,      # mini batch size\n",
    "    shuffle=True,               # 是否随机选择数据\n",
    "    num_workers=2,              # 在多线程中读取数据\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里data_iter的使用跟上一节中一样,让我们读取并输出第一个小批量数据样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1937,  0.2852],\n",
      "        [-1.1784,  0.0629],\n",
      "        [-0.1066, -0.8491],\n",
      "        [-0.9084,  1.7490],\n",
      "        [ 1.5382,  1.3554],\n",
      "        [ 1.5827,  0.0061],\n",
      "        [-2.3566,  0.3380],\n",
      "        [ 0.3400,  1.0401],\n",
      "        [ 1.8608,  0.8541],\n",
      "        [ 0.1557,  1.9575]]) \n",
      " tensor([ 2.8509,  1.6226,  6.8792, -3.5634,  2.6615,  7.3295, -1.6684,  1.3640,\n",
      "         5.0219, -2.1364])\n"
     ]
    }
   ],
   "source": [
    "for X, y in data_iter:\n",
    "    print(X, '\\n', y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.定义模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先,导入torch.nn模块。实际上,“nn”是neural networks(神经网网络)的缩写。该模块定义了大量神经网络层。之前我们已经使用过了autograd,而nn就是利用autograd来定义模型。\n",
    "\n",
    "nn的核心数据结构是Module,它是一个抽象概念,既可以表示神经网络中的某个层(layer),也可以表示一个包含很多层的神经网络。\n",
    "\n",
    "在实际使用中,最常见的做法是继承nn.Module,撰写自己的网络或者是层。\n",
    "\n",
    "一个nn.Module实例应该包含一些层以及返回输出的前向传播(forward)方法。\n",
    "\n",
    "下面先来用nn.Module实现一个线性回归模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearNet(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, n_feature):\n",
    "        super(LinearNet, self).__init__()      # 调用父函数进行初始化\n",
    "        self.linear = nn.Linear(n_feature, 1)  \n",
    "        # 函数原型：torch.nn.Linear(in_features, out_features, bias=True)\n",
    "    \n",
    "    # forward 定义前向传播\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "    \n",
    "net = LinearNet(num_inputs)\n",
    "print(net)   # 使用print打印出网络的结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以用nn.Sequential来更加方便地搭建网络,\n",
    "\n",
    "Sequential是一个有序的容器,网络层将按照传入Sequential的顺序依次被添加到计算图中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n",
      "Linear(in_features=2, out_features=1, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# 建立多层网络的方法\n",
    "# 方法一\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(num_inputs, 1)\n",
    "    # 此处还可以传入其他层\n",
    "    )\n",
    "\n",
    "# 方法二\n",
    "net = nn.Sequential()\n",
    "net.add_module('linear', nn.Linear(num_inputs, 1))\n",
    "# net.add_module ......\n",
    "\n",
    "# 方法三\n",
    "from collections import OrderedDict\n",
    "net = nn.Sequential(OrderedDict([\n",
    "          ('linear', nn.Linear(num_inputs, 1))\n",
    "          # ......\n",
    "        ]))\n",
    "\n",
    "print(net)\n",
    "print(net[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过net.parameters()来查看模型所有的可学习参数,此函数将返回一个生成器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.5347, 0.7057]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.6873], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作为一个单层神经网络,线性回归输出层中的神经元和输入层中各个输入完全连接。\n",
    "\n",
    "因此,线性回归的输出层又叫全连接层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.初始化模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用net前,我们需要初始化模型参数,如线性回归模型中的权重和偏差。\n",
    "\n",
    "PyTorch在 init 模块中提供了多种参数初始化方法。这里的init是initializer的缩写。\n",
    "\n",
    "我们通过init.normal_将每个权重参数初始化为随机采样于均值为0、标准差为0.01的正态分布。偏差会初始化为零。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import init\n",
    "\n",
    "init.normal_(net[0].weight, mean=0.0, std=0.01)\n",
    "init.constant_(net[0].bias, val=0.0)  # 或者可以使用net[0].bias.data.fill_(0)直接对bias的data进行修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.定义损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch在nn模块中提供了各种损失函数,这些损失函数可看作是一种特殊的层,\n",
    "\n",
    "PyTorch也将这些损失函数实现为nn.Module的子类。\n",
    "\n",
    "我们现在使用它提供的均方误差损失作为模型的损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()    # nn内置平方损失函数\n",
    "# 函数原型：`torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.定义优化函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样,我们也无须自己实现小批量随机梯度下降算法。\n",
    "\n",
    "torch.optim模块提供了很多常用的优化算法,比如SGD、Adam和RMSProp等。\n",
    "\n",
    "下面我们创建一个用于优化net所有参数的优化器实例,并指定学习率为0.03的小批量随机梯度下降(SGD)为优化算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.03\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.03)   # 内置随机梯度下降功能\n",
    "# 函数原型：torch.optim.SGD(params, lr=, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用Gluon训练模型时,我们通过调用optim实例的step函数来迭代模型参数。\n",
    "\n",
    "按照小批量随机梯度下降的定义,我们在step函数中指明批量大小,从而对批量中样本梯度求平均。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.000336\n",
      "epoch 2, loss: 0.000131\n",
      "epoch 3, loss: 0.000086\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    for X, y in data_iter:\n",
    "        output = net(X)\n",
    "        l = loss(output, y.view(-1, 1))\n",
    "        optimizer.zero_grad() # 梯度清零,等价于net.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    print('epoch %d, loss: %f' % (epoch, l.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们分别比较学到的模型参数和真实的模型参数的值。\n",
    "\n",
    "我们从net中获得需要的层,并访问其权重(weight)和偏差(bias)。\n",
    "\n",
    "得到的参数和真实的参数很接近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, -3.4] tensor([[ 2.0003, -3.3994]])\n",
      "4.2 tensor([4.2001])\n"
     ]
    }
   ],
   "source": [
    "# 结果比较\n",
    "dense = net[0]\n",
    "print(true_w, dense.weight.data)\n",
    "print(true_b, dense.bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "使用pytorch能够更加快速简洁地完成模型的设计与实现\n",
    "\n",
    "torch.utils.data模块提供了有关数据处理的工具;\n",
    "\n",
    "torch.nn模块定义了大量神经网络的层;\n",
    "\n",
    "torch.nn.init模块定义了各种初始化方法;\n",
    "\n",
    "torch.optim模块提供了模型参数初始化的各种方法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('pytorch-opencv': conda)",
   "language": "python",
   "name": "python37664bitpytorchopencvconda84e7148f32f3447aab8b4a7dabe30ed3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
