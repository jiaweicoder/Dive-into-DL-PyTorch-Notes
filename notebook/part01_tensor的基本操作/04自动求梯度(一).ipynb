{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bit20f34dd1b2ae42348a0cb3969f8b42ab",
   "display_name": "Python 3.7.4 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 自动求梯度(一)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将tensor的属性.requires_grad设置为True，开始追踪其上所有的操作\n",
    "\n",
    "完成一系列操作后，用.backward()完成所有的梯度计算；tensor的梯度将累积到.grad属性中\n",
    "\n",
    "用.detach()来停止追踪；也可以用with torch.no_grad()将不想追踪的代码块包裹起来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor与function结合起来可以创建一个记录有整个计算过程的有向无环图\n",
    "\n",
    "每个tensor都有一个.grad_fn属性\n",
    "\n",
    "若该tensor是由某个运算得到的，则.grad_fn返回一个与该运算过程有关的对象\n",
    "\n",
    "否则,.grad_fn返回None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[1., 1.],\n        [1., 1.]], requires_grad=True)\nNone\n"
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[3., 3.],\n        [3., 3.]], grad_fn=<AddBackward0>)\n<AddBackward0 object at 0x000001FA8DE9BB48>\n"
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "True\nFalse\n"
    }
   ],
   "source": [
    "# 像x这种是直接创建的，被称为叶子节点，叶子节点的grad_fn是None\n",
    "print(x.is_leaf)\n",
    "print(y.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[27., 27.],\n        [27., 27.]], grad_fn=<MulBackward0>)\ntensor(27., grad_fn=<MeanBackward1>)\n"
    }
   ],
   "source": [
    "# 复杂度运算\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过.requires_grad()的方法来改变requires_grad属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "False\nTrue\nTrue\n<SumBackward0 object at 0x000001FA8DEA4D88>\n"
    }
   ],
   "source": [
    "a = torch.randn(2, 2)   # 默认requires_grad=False\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.requires_grad)\n",
    "print(b.grad_fn)"
   ]
  }
 ]
}